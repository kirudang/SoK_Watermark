
# SoK: Are Watermarks in LLMs Ready for Deployment?

## Requirements
To set up the environment for this project, it's recommended to create a dedicated environment and install the required packages listed in `requirements.txt`.

```bash
conda create -n env python=3.10
conda activate env
pip install -r requirements.txt
```

## Experiments with the Cross-Model IP Classifier
All code used in our paper is located in the `src/` folder, with datasets available in the `data/` folder. Detailed instructions on reproducing the results or training the model with your dataset are provided below.

### Step 1: Watermark Implementation
To use the classifier, we first need input text generated by LLMs, along with the watermarked versions. Start by creating the training and testing datasets. It's advisable to label text generated from different LLMs or watermarks with a numeric identifier, e.g., 1, 2, 3.

For generating text with watermarks, follow the original settings specified in the following resources for each type of watermark:
- **KGW**: [KGW Repository](https://github.com/jwkirchenbauer/lm-watermarking)
- **EXP**: [EXP Repository](https://github.com/jthickstun/watermark)
- **SIR**: [SIR Repository](https://github.com/THU-BPM/Robust_Watermark)
- **SemStamp**: [SemStamp Repository](https://github.com/bohanhou14/SemStamp)

### Step 2: Classify Text using the IP Classifier
We provide four main types of classifiers:
1. Non-transformer using LSTM
2. Decoder-based transformer
3. Encoder-based transformer
4. Encoder-Decoder transformer

For each classifier type, you only need to update the model name and the method of calling the model. All parameters are clearly stated in the code.

### Data
- **Training data**: Available at `data/c4_promt_test.pt`
- **Testing data**: Available at `data/c4_promt_train.pt`

Feel free to explore and enjoy the code!
